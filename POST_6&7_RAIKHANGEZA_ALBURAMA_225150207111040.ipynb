{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jvonXDMsQP1k"
      },
      "source": [
        "# Backpropagation"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import time"
      ],
      "metadata": {
        "id": "YzH-ilE2twTF"
      },
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Pada post-test kali ini akan membandingkan dua jenis fungsi aktivasi yang biasa digunakan dalam backpropogation"
      ],
      "metadata": {
        "id": "-gGbt71BdLJV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Fungsi Aktivasi Sigmoid dengan turunannya\n",
        "def sig(X):\n",
        "  return [1 / (1 + np.exp(-x)) for x in X]\n",
        "\n",
        "def sigd(X):\n",
        "  output = []\n",
        "  for x in X:\n",
        "      s = sig([x])[0]\n",
        "      output.append(s * (1 - s))\n",
        "  return output\n",
        "\n",
        "#Fungsi Aktivasi Hyperbolic Tangent dengan turunannya\n",
        "def tanh(X):\n",
        "  return [np.tanh(x) for x in X]\n",
        "\n",
        "def tanhd(X):\n",
        "  output = []\n",
        "  for x in X:\n",
        "      t = tanh([x])[0]\n",
        "      output.append(1 - t**2)\n",
        "  return output"
      ],
      "metadata": {
        "id": "BTLa3NWvz7sq"
      },
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def onehot_enc(lbl, min_val=0):\n",
        "  mi = min(lbl)\n",
        "  enc = np.full((len(lbl), max(lbl) - mi + 1), min_val, np.int8)\n",
        "\n",
        "  for i, x in enumerate(lbl):\n",
        "    enc[i, x - mi] = 1\n",
        "\n",
        "  return enc\n",
        "\n",
        "def onehot_dec(enc, mi=0):\n",
        "  return [np.argmax(e) + mi for e in enc]"
      ],
      "metadata": {
        "id": "MopOydIkUjtH"
      },
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hihqFCY_ctZ3"
      },
      "source": [
        "### a) Fungsi *Training* Backpropagation\n",
        "\n",
        "Tulis kode ke dalam *cell* di bawah ini:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pTlk5igwcvc5"
      },
      "source": [
        "def bp_fit_sig(X, target, layer_conf, max_epoch, max_error=.1, learn_rate=.1, print_per_epoch=100):\n",
        "  start_time = time.time()\n",
        "  np.random.seed(1)\n",
        "  # Lengkapi kode Dibawah ini\n",
        "  # These lines were incorrectly indented\n",
        "  nin = [np.zeros(layer_conf[i]) for i in range(len(layer_conf))]  # net inputs\n",
        "  n = [np.zeros(layer_conf[i] + 1) for i in range(len(layer_conf))]  # activations (+1 for bias)\n",
        "  w = np.array([np.random.rand(layer_conf[i] + 1, layer_conf[i + 1]) for i in range(len(layer_conf) - 1)]) # This line was causing the error\n",
        "\n",
        "  dw = [np.empty((layer_conf[i] + 1, layer_conf[i + 1])) for i in range(len(layer_conf) - 1)]\n",
        "  d = [np.empty(s) for s in layer_conf[1:]]\n",
        "  din = [np.empty(s) for s in layer_conf[1:-1]]\n",
        "  epoch = 0\n",
        "  mse = 1\n",
        "  for i in range(0, len(n)-1):\n",
        "    n[i][-1] = 1\n",
        "  while (max_epoch == -1 or epoch < max_epoch) and mse > max_error:\n",
        "    epoch += 1\n",
        "    mse = 0\n",
        "    for r in range(len(X)):\n",
        "      n[0][:-1] = X[r]\n",
        "      for L in range(1, len(layer_conf)):\n",
        "        nin[L] = np.dot(n[L-1], w[L-1])\n",
        "        n[L][:len(nin[L])] = sig(nin[L])\n",
        "      e = target[r] - n[-1]\n",
        "      mse += sum(e ** 2)\n",
        "      d[-1] = e * sigd(nin[-1])\n",
        "      dw[-1] = learn_rate * d[-1] * n[-2].reshape((-1, 1))\n",
        "      for L in range(len(layer_conf) - 1, 1, -1):\n",
        "\n",
        "        # Lengkapi kode Dibawah ini\n",
        "        din[L-2] = np.dot(d[L-1], w[L-1][:-1].T)  # Calculate the gradient of the error for the previous layer\n",
        "        d[L-2] = din[L-2] * sigd(nin[L-2])       # Calculate the delta for the previous layer\n",
        "        dw[L-2] = learn_rate * np.outer(n[L-2], d[L-2])  # Weight update for the previous layer\n",
        "\n",
        "\n",
        "      w += dw\n",
        "    mse /= len(X)\n",
        "    if print_per_epoch > -1 and epoch % print_per_epoch == 0:\n",
        "      print(f'Epoch {epoch}, MSE: {mse}')\n",
        "  execution = time.time() - start_time\n",
        "  print(\"Waktu eksekusi: %s detik\" % execution)\n",
        "  return w, epoch, mse"
      ],
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def bp_fit_tanh(X, target, layer_conf, max_epoch, max_error=.1, learn_rate=.1, print_per_epoch=100):\n",
        "  start_time = time.time()\n",
        "  np.random.seed(1)\n",
        "  # Initialize weights, activations, and other variables similar to bp_fit_sig\n",
        "  nin = [np.zeros(layer_conf[i]) for i in range(len(layer_conf))]\n",
        "  n = [np.zeros(layer_conf[i] + 1) for i in range(len(layer_conf))]\n",
        "  w = np.array([np.random.rand(layer_conf[i] + 1, layer_conf[i + 1]) for i in range(len(layer_conf) - 1)])\n",
        "  dw = [np.empty((layer_conf[i] + 1, layer_conf[i + 1])) for i in range(len(layer_conf) - 1)]\n",
        "  d = [np.empty(s) for s in layer_conf[1:]]\n",
        "  din = [np.empty(s) for s in layer_conf[1:-1]]\n",
        "  epoch = 0\n",
        "  mse = 1\n",
        "\n",
        "  # Set bias nodes to 1\n",
        "  for i in range(0, len(n)-1):\n",
        "    n[i][-1] = 1\n",
        "\n",
        "  # Training loop\n",
        "  while (max_epoch == -1 or epoch < max_epoch) and mse > max_error:\n",
        "    epoch += 1\n",
        "    mse = 0\n",
        "    for r in range(len(X)):\n",
        "      n[0][:-1] = X[r]\n",
        "\n",
        "      # Forward propagation\n",
        "      for L in range(1, len(layer_conf)):\n",
        "        nin[L] = np.dot(n[L-1], w[L-1])\n",
        "        n[L][:len(nin[L])] = tanh(nin[L])  # Use tanh activation\n",
        "\n",
        "      # Calculate error and deltas\n",
        "      e = target[r] - n[-1]\n",
        "      mse += sum(e ** 2)\n",
        "      d[-1] = e * tanhd(nin[-1])  # Use tanhd for derivative\n",
        "      dw[-1] = learn_rate * d[-1] * n[-2].reshape((-1, 1))\n",
        "\n",
        "      # Backpropagation\n",
        "      for L in range(len(layer_conf) - 1, 1, -1):\n",
        "        din[L-2] = np.dot(d[L-1], w[L-1][:-1].T)\n",
        "        d[L-2] = din[L-2] * tanhd(nin[L-2])  # Use tanhd for derivative\n",
        "        dw[L-2] = learn_rate * np.outer(n[L-2], d[L-2])\n",
        "\n",
        "      # Update weights\n",
        "      w += dw\n",
        "\n",
        "    mse /= len(X)\n",
        "    if print_per_epoch > -1 and epoch % print_per_epoch == 0:\n",
        "      print(f'Epoch {epoch}, MSE: {mse}')\n",
        "  execution = time.time() - start_time\n",
        "  print(\"Waktu eksekusi: %s detik\" % execution)\n",
        "  return w, epoch, mse"
      ],
      "metadata": {
        "id": "Pet6ptVOTxUR"
      },
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eJA_9btdc3ED"
      },
      "source": [
        "### b) Fungsi *Testing* Backpropagation\n",
        "\n",
        "Tulis kode ke dalam *cell* di bawah ini:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2zyXIu_ec9go"
      },
      "source": [
        "def bp_predict_sig(X, w):\n",
        "  n = [np.empty(len(i)) for i in w]\n",
        "  nin = [np.empty(len(i[0])) for i in w]\n",
        "  predict = []\n",
        "  n.append(np.empty(len(w[-1][0])))\n",
        "  for x in X:\n",
        "    n[0][:-1] = x\n",
        "    for L in range(0, len(w)):\n",
        "      nin[L] = np.dot(n[L], w[L])\n",
        "      n[L + 1][:len(nin[L])] = sig(nin[L])\n",
        "    predict.append(n[-1].copy())\n",
        "  return predict"
      ],
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def bp_predict_tanh(X, w):\n",
        "  n = [np.empty(len(i)) for i in w]\n",
        "  nin = [np.empty(len(i[0])) for i in w]\n",
        "  predict = []\n",
        "  n.append(np.empty(len(w[-1][0])))\n",
        "  for x in X:\n",
        "    n[0][:-1] = x\n",
        "    for L in range(0, len(w)):\n",
        "      nin[L] = np.dot(n[L], w[L])\n",
        "      n[L + 1][:len(nin[L])] = tanh(nin[L])  # Use tanh activation\n",
        "    predict.append(n[-1].copy())\n",
        "  return predict"
      ],
      "metadata": {
        "id": "paHySilia3gw"
      },
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PZxy_M5Jc-ko"
      },
      "source": [
        "### c) Klasifikasi dataset wine\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Lakukan pelatihan pada dataset wine dengan menggunakan 2 fungsi pelatihan yang telah dibuat!\n",
        "\n",
        "Konfigurasi kedua pelatihan harus sama (epoch, hidden layer, learning rate, dll).\n",
        "Akurasi yang diharapkan di setiap pelatihan adalah > 0.98"
      ],
      "metadata": {
        "id": "4xj7DqCdudcF"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hw1L_Q3JdHk7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 493
        },
        "outputId": "96b9a66f-f7e8-4585-c560-57b4637d11f8"
      },
      "source": [
        "from sklearn import datasets\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import minmax_scale\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "wine = datasets.load_wine()\n",
        "X = minmax_scale(wine.data)\n",
        "Y = onehot_enc(wine.target)\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, Y,\n",
        "test_size=.3,random_state=1)\n",
        "#Isi jumlah layer yang digunakan dengan jumlah hidden layer #\n",
        "w, ep, mse = bp_fit_sig(X_train, y_train, layer_conf=(13, 4, 3),\n",
        "                        learn_rate=0.1, max_epoch=1000, max_error=0.1, print_per_epoch=25)\n",
        "\n",
        "print(f'Epochs: {ep}, MSE: {mse}')\n",
        "\n",
        "predict = bp_predict_sig(X_test, w)\n",
        "predict = onehot_dec(predict)\n",
        "y_test = onehot_dec(y_test)\n",
        "accuracy = accuracy_score(predict, y_test)\n",
        "\n",
        "print('Output:', predict)\n",
        "print('True :', y_test)\n",
        "print('Accuracy:', accuracy)"
      ],
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (2,) + inhomogeneous part.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-68-f9a0cc323e71>\u001b[0m in \u001b[0;36m<cell line: 13>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m test_size=.3,random_state=1)\n\u001b[1;32m     12\u001b[0m \u001b[0;31m#Isi jumlah layer yang digunakan dengan jumlah hidden layer #\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m w, ep, mse = bp_fit_sig(X_train, y_train, layer_conf=(13, 4, 3),\n\u001b[0m\u001b[1;32m     14\u001b[0m                         learn_rate=0.1, max_epoch=1000, max_error=0.1, print_per_epoch=25)\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-61-8c1c436c282e>\u001b[0m in \u001b[0;36mbp_fit_sig\u001b[0;34m(X, target, layer_conf, max_epoch, max_error, learn_rate, print_per_epoch)\u001b[0m\n\u001b[1;32m      6\u001b[0m   \u001b[0mnin\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlayer_conf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlayer_conf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m  \u001b[0;31m# net inputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m   \u001b[0mn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlayer_conf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlayer_conf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m  \u001b[0;31m# activations (+1 for bias)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m   \u001b[0mw\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrand\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlayer_conf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlayer_conf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlayer_conf\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# This line was causing the error\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m   \u001b[0mdw\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mempty\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlayer_conf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlayer_conf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlayer_conf\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (2,) + inhomogeneous part."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn import datasets\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import minmax_scale\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "wine = datasets.load_wine()\n",
        "X = minmax_scale(wine.data)\n",
        "Y = onehot_enc(wine.target)\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, Y,\n",
        "test_size=.3,random_state=1)\n",
        "#Isi jumlah layer yang digunakan dengan jumlah hidden layer #\n",
        "w, ep, mse = bp_fit_tanh(X_train, y_train, layer_conf=(13, 8, 3),\n",
        "learn_rate=0.1, max_epoch=1000, max_error=0.1, print_per_epoch=25)\n",
        "\n",
        "\n",
        "print(f'Epochs: {ep}, MSE: {mse}')\n",
        "\n",
        "predict = bp_predict_tanh(X_test, w)\n",
        "predict = onehot_dec(predict)\n",
        "y_test = onehot_dec(y_test)\n",
        "accuracy = accuracy_score(predict, y_test)\n",
        "\n",
        "print('Output:', predict)\n",
        "print('True :', y_test)\n",
        "print('Accuracy:', accuracy)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 219
        },
        "id": "mF18HdQgbCXy",
        "outputId": "abdc12db-f3be-4b4a-d4d7-ebd7708b81b2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "cannot unpack non-iterable NoneType object",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-52-6ade1894b9c7>\u001b[0m in \u001b[0;36m<cell line: 13>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m test_size=.3,random_state=1)\n\u001b[1;32m     12\u001b[0m \u001b[0;31m#Isi jumlah layer yang digunakan dengan jumlah hidden layer #\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m w, ep, mse = bp_fit_tanh(X_train, y_train, layer_conf=(13, 8, 3), \n\u001b[0m\u001b[1;32m     14\u001b[0m learn_rate=0.1, max_epoch=1000, max_error=0.1, print_per_epoch=25)\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: cannot unpack non-iterable NoneType object"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Pertanyaan"
      ],
      "metadata": {
        "id": "Lp6y7VWfjVEX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1.  Apa perbedaan dari penggunaan fungsi aktivasi sigmoid dengan fungsi aktivasi hyperbolic tangent?\n",
        "2. Coba jelaskan alasan dari perbedaan tersebut sebisa kalian"
      ],
      "metadata": {
        "id": "mP9dzq-kin0y"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Fv3vuV_9euD8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Jawaban"
      ],
      "metadata": {
        "id": "RHEJApRcjXu7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **1**.  Oke, berikut ringkasan perbedaan fungsi aktivasi sigmoid dan hyperbolic tangent (tanh):\n",
        "\n",
        "Perbedaan:\n",
        "\n",
        "Rentang Output: Sigmoid menghasilkan output antara 0 dan 1, sedangkan tanh menghasilkan output antara -1 dan 1.\n",
        "Sentrisitas: Tanh bersifat centered di 0, sedangkan sigmoid tidak. Artinya, output tanh berpusat di sekitar 0, sedangkan output sigmoid berpusat di sekitar 0,5.\n",
        "Bias Shift: Tanh mengurangi bias shift dibandingkan sigmoid karena sentrisitasnya. Ini dapat mempercepat konvergensi selama training.\n",
        "Gradient Vanishing: Sigmoid lebih rentan terhadap vanishing gradient, di mana gradien menjadi sangat kecil dan menghambat training. Tanh sedikit mengurangi masalah ini karena rentang output yang lebih luas.\n",
        "\n",
        "2.  Formula tanh menghasilkan output yang berpusat di sekitar 0 dan memiliki rentang output yang lebih luas. Hal ini memberikan keuntungan dalam hal mengurangi bias shift dan vanishing gradient."
      ],
      "metadata": {
        "id": "4S55HVfLjaZ5"
      }
    }
  ]
}